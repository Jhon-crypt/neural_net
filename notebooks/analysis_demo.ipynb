{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# MNIST Model Analysis & Evaluation\n",
        "\n",
        "This notebook provides comprehensive analysis of our trained MNIST classifier, including detailed performance metrics, error analysis, and visualization of model behavior.\n",
        "\n",
        "## Analysis Overview\n",
        "- Load pre-trained model\n",
        "- Detailed performance evaluation\n",
        "- Confusion matrix analysis\n",
        "- Error analysis and misclassified examples\n",
        "- Custom image testing\n",
        "- Model interpretation and insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## Setup and Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('../src')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from mnist_classifier import MNISTClassifier\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize classifier and load pre-trained model\n",
        "classifier = MNISTClassifier()\n",
        "classifier.load_data()\n",
        "\n",
        "# Try to load existing model, or train a new one if needed\n",
        "try:\n",
        "    classifier.load_model(\"../models/mnist_model.keras\")\n",
        "    print(\"‚úÖ Loaded pre-trained model!\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è No pre-trained model found. Please run training_demo.ipynb first.\")\n",
        "    print(\"For demo purposes, we'll build and do a quick train...\")\n",
        "    classifier.build_model()\n",
        "    classifier.train(epochs=5, batch_size=128)\n",
        "    \n",
        "print(f\"Model loaded and ready for analysis!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Comprehensive Performance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions and evaluate\n",
        "predictions = classifier.model.predict(classifier.x_test)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = np.argmax(classifier.y_test, axis=1)\n",
        "prediction_confidences = np.max(predictions, axis=1)\n",
        "\n",
        "# Overall performance\n",
        "test_loss, test_accuracy = classifier.evaluate()\n",
        "print(\"üéØ OVERALL PERFORMANCE\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Total Test Samples: {len(true_classes)}\")\n",
        "print(f\"Correct Predictions: {np.sum(predicted_classes == true_classes)}\")\n",
        "print(f\"Incorrect Predictions: {np.sum(predicted_classes != true_classes)}\")\n",
        "\n",
        "# Confidence statistics\n",
        "print(f\"\\nüìä CONFIDENCE STATISTICS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Mean Confidence: {np.mean(prediction_confidences):.4f}\")\n",
        "print(f\"Median Confidence: {np.median(prediction_confidences):.4f}\")\n",
        "print(f\"Min Confidence: {np.min(prediction_confidences):.4f}\")\n",
        "print(f\"Max Confidence: {np.max(prediction_confidences):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Confusion Matrix & Classification Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrix\n",
        "classifier.plot_confusion_matrix(true_classes, predicted_classes)\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"üìã DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(true_classes, predicted_classes, \n",
        "                          target_names=[f'Digit {i}' for i in range(10)]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Error Analysis - Misclassified Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find misclassified examples\n",
        "import tensorflow as tf\n",
        "(x_train_orig, y_train_orig), (x_test_orig, y_test_orig) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "misclassified_indices = np.where(predicted_classes != true_classes)[0]\n",
        "print(f\"üîç Found {len(misclassified_indices)} misclassified examples\")\n",
        "\n",
        "# Show worst predictions (lowest confidence among errors)\n",
        "if len(misclassified_indices) > 0:\n",
        "    error_confidences = prediction_confidences[misclassified_indices]\n",
        "    worst_errors_idx = misclassified_indices[np.argsort(error_confidences)[:12]]\n",
        "    \n",
        "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
        "    fig.suptitle('Most Difficult Misclassified Examples (Lowest Confidence)', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    for i, idx in enumerate(worst_errors_idx):\n",
        "        row, col = i // 4, i % 4\n",
        "        axes[row, col].imshow(x_test_orig[idx], cmap='gray')\n",
        "        axes[row, col].set_title(f'True: {true_classes[idx]}, Pred: {predicted_classes[idx]}\\\\nConf: {prediction_confidences[idx]:.3f}', \n",
        "                                color='red', fontweight='bold')\n",
        "        axes[row, col].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"üéâ Perfect classification! No errors found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Sample Predictions Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample predictions\n",
        "classifier.visualize_predictions(num_samples=10)\n",
        "\n",
        "print(\"üéâ Analysis completed!\")\n",
        "print(\"\\\\nüìù Summary:\")\n",
        "print(f\"- Model achieves {test_accuracy*100:.2f}% accuracy on test set\")\n",
        "print(f\"- {len(misclassified_indices)} out of {len(true_classes)} examples misclassified\")\n",
        "print(f\"- Average prediction confidence: {np.mean(prediction_confidences):.3f}\")\n",
        "print(\"\\\\n‚úÖ The model shows excellent performance on MNIST digit classification!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
